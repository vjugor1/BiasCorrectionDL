{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    \"\"\"### Up block This combines `ResidualBlock` and `AttentionBlock`.\n",
    "\n",
    "    These are used in the second half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        has_attn: bool = False,\n",
    "        activation: str = \"leaky\",\n",
    "        norm: bool = False,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n",
    "        # from the first half of the U-Net\n",
    "        self.res = ResidualBlock(\n",
    "            in_channels + out_channels,\n",
    "            out_channels,\n",
    "            activation=activation,\n",
    "            norm=norm,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.res(x)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MiddleBlock(nn.Module):\n",
    "    \"\"\"### Middle block It combines a `ResidualBlock`, `AttentionBlock`, followed by another\n",
    "    `ResidualBlock`.\n",
    "\n",
    "    This block is applied at the lowest resolution of the U-Net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_channels: int,\n",
    "        has_attn: bool = False,\n",
    "        activation: str = \"leaky\",\n",
    "        norm: bool = False,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(\n",
    "            n_channels,\n",
    "            n_channels,\n",
    "            activation=activation,\n",
    "            norm=norm,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.attn = AttentionBlock(n_channels) if has_attn else nn.Identity()\n",
    "        self.res2 = ResidualBlock(\n",
    "            n_channels,\n",
    "            n_channels,\n",
    "            activation=activation,\n",
    "            norm=norm,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.res1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.res2(x)\n",
    "        return x\n",
    "class PeriodicPadding2D(nn.Module):\n",
    "    def __init__(self, pad_width, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_width = pad_width\n",
    "\n",
    "    def forward(self, inputs, **kwargs):\n",
    "        if self.pad_width == 0:\n",
    "            return inputs\n",
    "        inputs_padded = torch.cat(\n",
    "            (\n",
    "                inputs[:, :, :, -self.pad_width :],\n",
    "                inputs,\n",
    "                inputs[:, :, :, : self.pad_width],\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "        # Zero padding in the lat direction\n",
    "        inputs_padded = nn.functional.pad(\n",
    "            inputs_padded, (0, 0, self.pad_width, self.pad_width)\n",
    "        )\n",
    "        return inputs_padded\n",
    "    \n",
    "class PeriodicConv2D(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, stride=1, padding=0, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.padding = PeriodicPadding2D(padding)\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=0\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.conv(self.padding(inputs))\n",
    "    \n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"### Scale down the feature map by $\\frac{1}{2} \\times$\"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.conv(x)\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"### Down block This combines `ResidualBlock` and `AttentionBlock`.\n",
    "\n",
    "    These are used in the first half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        has_attn: bool = False,\n",
    "        activation: str = \"leaky\",\n",
    "        norm: bool = False,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.res = ResidualBlock(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            activation=activation,\n",
    "            norm=norm,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.res(x)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        activation: str = \"leaky\",\n",
    "        norm: bool = False,\n",
    "        dropout: float = 0.1,\n",
    "        n_groups: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if activation == \"gelu\":\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == \"silu\":\n",
    "            self.activation = nn.SiLU()\n",
    "        elif activation == \"leaky\":\n",
    "            self.activation = nn.LeakyReLU(0.3)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Activation {activation} not implemented\")\n",
    "\n",
    "        self.conv1 = PeriodicConv2D(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = PeriodicConv2D(\n",
    "            out_channels, out_channels, kernel_size=3, padding=1\n",
    "        )\n",
    "        # If the number of input channels is not equal to the number of output channels we have to\n",
    "        # project the shortcut connection\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        if norm:\n",
    "            self.norm1 = nn.BatchNorm2d(out_channels)\n",
    "            self.norm2 = nn.BatchNorm2d(out_channels)\n",
    "        else:\n",
    "            self.norm1 = nn.Identity()\n",
    "            self.norm2 = nn.Identity()\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # First convolution layer\n",
    "        # h = self.drop(self.conv1(self.activation(self.norm1(x))))\n",
    "        h = self.drop(self.norm1(self.activation(self.conv1(x))))\n",
    "        # Second convolution layer\n",
    "        # h = self.drop(self.conv2(self.activation(self.norm2(h))))\n",
    "        h = self.drop(self.norm2(self.activation(self.conv2(h))))\n",
    "        # Add the shortcut connection and return\n",
    "        return h + self.shortcut(x)\n",
    "    \n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"### Attention block This is similar to [transformer multi-head\n",
    "    attention](../../transformers/mha.html).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of channels in the input\n",
    "        * `n_heads` is the number of heads in multi-head attention\n",
    "        * `d_k` is the number of dimensions in each head\n",
    "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Default `d_k`\n",
    "        if d_k is None:\n",
    "            d_k = n_channels\n",
    "        # Normalization layer\n",
    "        self.norm = nn.BatchNorm2d(n_channels)\n",
    "        # Projections for query, key and values\n",
    "        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n",
    "        # Linear layer for final transformation\n",
    "        self.output = nn.Linear(n_heads * d_k, n_channels)\n",
    "        # Scale for dot-product attention\n",
    "        self.scale = d_k**-0.5\n",
    "        #\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Get shape\n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        # Change `x` to shape `[batch_size, seq, n_channels]`\n",
    "        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
    "        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n",
    "        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n",
    "        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "        attn = torch.einsum(\"bihd,bjhd->bijh\", q, k) * self.scale\n",
    "        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
    "        attn = attn.softmax(dim=1)\n",
    "        # Multiply by values\n",
    "        res = torch.einsum(\"bijh,bjhd->bihd\", attn, v)\n",
    "        # Reshape to `[batch_size, seq, n_heads * d_k]`\n",
    "        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        # Transform to `[batch_size, seq, n_channels]`\n",
    "        res = self.output(res)\n",
    "\n",
    "        # Add skip connection\n",
    "        res += x\n",
    "\n",
    "        # Change to shape `[batch_size, in_channels, height, width]`\n",
    "        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
    "        return res\n",
    "    \n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"### Scale up the feature map by $2 \\times$\"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 3\n",
    "out_channels = 3\n",
    "history=1\n",
    "hidden_channels=64\n",
    "activation=\"leaky\"\n",
    "norm = True\n",
    "dropout= 0.1\n",
    "ch_mults = (1, 2, 2, 4)\n",
    "is_attn = (False, False, False, False)\n",
    "mid_attn = False\n",
    "n_blocks= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_proj = PeriodicConv2D(\n",
    "            in_channels, hidden_channels, kernel_size=7, padding=3\n",
    "        )\n",
    "\n",
    "# #### First half of U-Net - decreasing resolution\n",
    "down = []\n",
    "# Number of channels\n",
    "out_channels = in_channels = hidden_channels\n",
    "# For each resolution\n",
    "n_resolutions = len(ch_mults)\n",
    "for i in range(n_resolutions):\n",
    "    # Number of output channels at this resolution\n",
    "    out_channels = in_channels * ch_mults[i]\n",
    "    # Add `n_blocks`\n",
    "    for _ in range(n_blocks):\n",
    "        down.append(\n",
    "            DownBlock(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                has_attn=is_attn[i],\n",
    "                activation=activation,\n",
    "                norm=norm,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        )\n",
    "        in_channels = out_channels\n",
    "    # Down sample at all resolutions except the last\n",
    "    if i < n_resolutions - 1:\n",
    "        down.append(Downsample(in_channels))\n",
    "\n",
    "# Combine the set of modules\n",
    "down = nn.ModuleList(down)\n",
    "\n",
    "# Middle block\n",
    "middle = MiddleBlock(\n",
    "    out_channels,\n",
    "    has_attn=mid_attn,\n",
    "    activation=activation,\n",
    "    norm=norm,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "# #### Second half of U-Net - increasing resolution\n",
    "up = []\n",
    "# Number of channels\n",
    "in_channels = out_channels\n",
    "# For each resolution\n",
    "for i in reversed(range(n_resolutions)):\n",
    "    # `n_blocks` at the same resolution\n",
    "    out_channels = in_channels\n",
    "    for _ in range(n_blocks):\n",
    "        up.append(\n",
    "            UpBlock(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                has_attn=is_attn[i],\n",
    "                activation=activation,\n",
    "                norm=norm,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        )\n",
    "    # Final block to reduce the number of channels\n",
    "    out_channels = in_channels // ch_mults[i]\n",
    "    up.append(\n",
    "        UpBlock(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            has_attn=is_attn[i],\n",
    "            activation=activation,\n",
    "            norm=norm,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "    )\n",
    "    in_channels = out_channels\n",
    "    # Up sample at all resolutions except last\n",
    "    if i > 0:\n",
    "        up.append(Upsample(in_channels))\n",
    "\n",
    "# Combine the set of modules\n",
    "up = nn.ModuleList(up)\n",
    "\n",
    "if norm:\n",
    "    norm = nn.BatchNorm2d(hidden_channels)\n",
    "else:\n",
    "    norm = nn.Identity()\n",
    "final = PeriodicConv2D(\n",
    "    in_channels, 3, kernel_size=7, padding=3\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpBlock(\n",
       "  (res): ResidualBlock(\n",
       "    (activation): LeakyReLU(negative_slope=0.3)\n",
       "    (conv1): PeriodicConv2D(\n",
       "      (padding): PeriodicPadding2D()\n",
       "      (conv): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (conv2): PeriodicConv2D(\n",
       "      (padding): PeriodicPadding2D()\n",
       "      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (shortcut): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (norm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (attn): Identity()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "up[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(x.shape) == 5:  # x.shape = [B,T,C,H,W]\n",
    "    x = x.flatten(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image_proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 64])\n",
      "torch.Size([1, 64, 64, 64])\n",
      "torch.Size([1, 64, 32, 32])\n",
      "torch.Size([1, 128, 32, 32])\n",
      "torch.Size([1, 128, 32, 32])\n",
      "torch.Size([1, 128, 16, 16])\n",
      "torch.Size([1, 256, 16, 16])\n",
      "torch.Size([1, 256, 16, 16])\n",
      "torch.Size([1, 256, 8, 8])\n",
      "torch.Size([1, 1024, 8, 8])\n",
      "torch.Size([1, 1024, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "for m in down:\n",
    "    x = m(x)\n",
    "    print(x.shape)\n",
    "    h.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "x = middle(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Upsample(\n",
       "  (conv): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "up[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 8, 8])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 8, 8])\n",
      "torch.Size([1, 2048, 8, 8])\n",
      "torch.Size([1, 1024, 8, 8])\n",
      "torch.Size([1, 1280, 8, 8])\n",
      "torch.Size([1, 256, 8, 8])\n",
      "torch.Size([1, 256, 8, 8])\n",
      "torch.Size([1, 256, 16, 16])\n",
      "torch.Size([1, 512, 16, 16])\n",
      "torch.Size([1, 256, 16, 16])\n",
      "torch.Size([1, 512, 16, 16])\n",
      "torch.Size([1, 256, 16, 16])\n",
      "torch.Size([1, 384, 16, 16])\n",
      "torch.Size([1, 128, 16, 16])\n",
      "torch.Size([1, 128, 16, 16])\n",
      "torch.Size([1, 128, 32, 32])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 128, 32, 32])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 128, 32, 32])\n",
      "torch.Size([1, 192, 32, 32])\n",
      "torch.Size([1, 64, 32, 32])\n",
      "torch.Size([1, 64, 32, 32])\n",
      "torch.Size([1, 64, 64, 64])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 64, 64, 64])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 64, 64, 64])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for m in up:\n",
    "    if isinstance(m, Upsample):\n",
    "        print(x.shape)\n",
    "        x = m(x)\n",
    "        print(x.shape)\n",
    "    else:\n",
    "        # Get the skip connection from first half of U-Net and concatenate\n",
    "        s = h.pop()\n",
    "        x = torch.cat((x, s), dim=1)\n",
    "        print(x.shape)\n",
    "        x = m(x)\n",
    "        print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = final(nn.ReLU()(norm(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m PeriodicPadding2D(\u001b[38;5;241m3\u001b[39m)(sample)[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m] \u001b[38;5;241m==\u001b[39m PeriodicPadding2D(\u001b[38;5;241m3\u001b[39m)(sample)[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,:,\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "PeriodicPadding2D(3)(sample)[0,1,:,-6] == PeriodicPadding2D(3)(sample)[0,1,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 70, 70])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PeriodicPadding2D(3)(sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_proj = PeriodicConv2D(3, 64, kernel_size=7, padding=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = img_proj(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 64])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bias_correction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
