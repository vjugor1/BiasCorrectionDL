{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2D sine-cosine position embedding\n",
    "# References:\n",
    "# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n",
    "# MoCo v3: https://github.com/facebookresearch/moco-v3\n",
    "# --------------------------------------------------------\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size_h, grid_size_w, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size_h, dtype=float)\n",
    "    grid_w = np.arange(grid_size_w, dtype=float)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size_h, grid_size_w])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.0\n",
    "    omega = 1.0 / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum(\"m,d->md\", pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Interpolate position embeddings for high-resolution\n",
    "# References:\n",
    "# DeiT: https://github.com/facebookresearch/deit\n",
    "# --------------------------------------------------------\n",
    "def interpolate_pos_embed(model, checkpoint_model):\n",
    "    if \"pos_embed\" in checkpoint_model:\n",
    "        pos_embed_checkpoint = checkpoint_model[\"pos_embed\"]\n",
    "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
    "        num_patches = model.patch_embed.num_patches\n",
    "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
    "        # height (== width) for the checkpoint position embedding\n",
    "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
    "        # height (== width) for the new position embedding\n",
    "        new_size = int(num_patches**0.5)\n",
    "        # class_token and dist_token are kept unchanged\n",
    "        if orig_size != new_size:\n",
    "            print(\n",
    "                \"Position interpolate from %dx%d to %dx%d\"\n",
    "                % (orig_size, orig_size, new_size, new_size)\n",
    "            )\n",
    "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
    "            # only the position tokens are interpolated\n",
    "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
    "            pos_tokens = pos_tokens.reshape(\n",
    "                -1, orig_size, orig_size, embedding_size\n",
    "            ).permute(0, 3, 1, 2)\n",
    "            pos_tokens = torch.nn.functional.interpolate(\n",
    "                pos_tokens,\n",
    "                size=(new_size, new_size),\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
    "            checkpoint_model[\"pos_embed\"] = new_pos_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from typing import Callable, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.jit import Final\n",
    "\n",
    "from timm.layers import PatchEmbed, Mlp, DropPath, trunc_normal_, lecun_normal_, resample_patch_embed, \\\n",
    "    resample_abs_pos_embed, RmsNorm, PatchDropout, use_fused_attn, SwiGLUPacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicPadding2D(nn.Module):\n",
    "    def __init__(self, pad_width, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_width = pad_width\n",
    "\n",
    "    def forward(self, inputs, **kwargs):\n",
    "        if self.pad_width == 0:\n",
    "            return inputs\n",
    "        inputs_padded = torch.cat(\n",
    "            (\n",
    "                inputs[:, :, :, -self.pad_width :],\n",
    "                inputs,\n",
    "                inputs[:, :, :, : self.pad_width],\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "        # Zero padding in the lat direction\n",
    "        inputs_padded = nn.functional.pad(\n",
    "            inputs_padded, (0, 0, self.pad_width, self.pad_width)\n",
    "        )\n",
    "        return inputs_padded\n",
    "\n",
    "\n",
    "class PeriodicConv2D(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, stride=1, padding=0, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.padding = PeriodicPadding2D(padding)\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=0\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.conv(self.padding(inputs))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        activation: str = \"leaky\",\n",
    "        norm: bool = False,\n",
    "        dropout: float = 0.1,\n",
    "        n_groups: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if activation == \"gelu\":\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == \"silu\":\n",
    "            self.activation = nn.SiLU()\n",
    "        elif activation == \"leaky\":\n",
    "            self.activation = nn.LeakyReLU(0.3)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Activation {activation} not implemented\")\n",
    "\n",
    "        self.conv1 = PeriodicConv2D(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = PeriodicConv2D(\n",
    "            out_channels, out_channels, kernel_size=3, padding=1\n",
    "        )\n",
    "        # If the number of input channels is not equal to the number of output channels we have to\n",
    "        # project the shortcut connection\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        if norm:\n",
    "            self.norm1 = nn.BatchNorm2d(out_channels)\n",
    "            self.norm2 = nn.BatchNorm2d(out_channels)\n",
    "        else:\n",
    "            self.norm1 = nn.Identity()\n",
    "            self.norm2 = nn.Identity()\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # First convolution layer\n",
    "        # h = self.drop(self.conv1(self.activation(self.norm1(x))))\n",
    "        h = self.drop(self.norm1(self.activation(self.conv1(x))))\n",
    "        # Second convolution layer\n",
    "        # h = self.drop(self.conv2(self.activation(self.norm2(h))))\n",
    "        h = self.drop(self.norm2(self.activation(self.conv2(h))))\n",
    "        # Add the shortcut connection and return\n",
    "        return h + self.shortcut(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    fused_attn: Final[bool]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads=8,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            attn_drop=0.,\n",
    "            proj_drop=0.,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.fused_attn = use_fused_attn()\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "        if self.fused_attn:\n",
    "            x = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p=self.attn_drop.p,\n",
    "            )\n",
    "        else:\n",
    "            q = q * self.scale\n",
    "            attn = q @ k.transpose(-2, -1)\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, init_values=1e-5, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            proj_drop=0.,\n",
    "            attn_drop=0.,\n",
    "            init_values=None,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            mlp_layer=Mlp,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_norm=qk_norm,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=proj_drop,\n",
    "            norm_layer=norm_layer,\n",
    "        )\n",
    "        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = mlp_layer(\n",
    "            in_features=dim,\n",
    "            hidden_features=int(dim * mlp_ratio),\n",
    "            act_layer=act_layer,\n",
    "            drop=proj_drop,\n",
    "        )\n",
    "        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResPostBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            proj_drop=0.,\n",
    "            attn_drop=0.,\n",
    "            init_values=None,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            mlp_layer=Mlp,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_values = init_values\n",
    "\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_norm=qk_norm,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=proj_drop,\n",
    "            norm_layer=norm_layer,\n",
    "        )\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.mlp = mlp_layer(\n",
    "            in_features=dim,\n",
    "            hidden_features=int(dim * mlp_ratio),\n",
    "            act_layer=act_layer,\n",
    "            drop=proj_drop,\n",
    "        )\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # NOTE this init overrides that base model init with specific changes for the block type\n",
    "        if self.init_values is not None:\n",
    "            nn.init.constant_(self.norm1.weight, self.init_values)\n",
    "            nn.init.constant_(self.norm2.weight, self.init_values)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.norm1(self.attn(x)))\n",
    "        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ParallelScalingBlock(nn.Module):\n",
    "    \"\"\" Parallel ViT block (MLP & Attention in parallel)\n",
    "    Based on:\n",
    "      'Scaling Vision Transformers to 22 Billion Parameters` - https://arxiv.org/abs/2302.05442\n",
    "    \"\"\"\n",
    "    fused_attn: Final[bool]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            proj_drop=0.,\n",
    "            attn_drop=0.,\n",
    "            init_values=None,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            mlp_layer=None,  # NOTE: not used\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.fused_attn = use_fused_attn()\n",
    "        mlp_hidden_dim = int(mlp_ratio * dim)\n",
    "        in_proj_out_dim = mlp_hidden_dim + 3 * dim\n",
    "\n",
    "        self.in_norm = norm_layer(dim)\n",
    "        self.in_proj = nn.Linear(dim, in_proj_out_dim, bias=qkv_bias)\n",
    "        self.in_split = [mlp_hidden_dim] + [dim] * 3\n",
    "        if qkv_bias:\n",
    "            self.register_buffer('qkv_bias', None)\n",
    "            self.register_parameter('mlp_bias', None)\n",
    "        else:\n",
    "            self.register_buffer('qkv_bias', torch.zeros(3 * dim), persistent=False)\n",
    "            self.mlp_bias = nn.Parameter(torch.zeros(mlp_hidden_dim))\n",
    "\n",
    "        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.attn_out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.mlp_drop = nn.Dropout(proj_drop)\n",
    "        self.mlp_act = act_layer()\n",
    "        self.mlp_out_proj = nn.Linear(mlp_hidden_dim, dim)\n",
    "\n",
    "        self.ls = LayerScale(dim, init_values=init_values) if init_values is not None else nn.Identity()\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # Combined MLP fc1 & qkv projections\n",
    "        y = self.in_norm(x)\n",
    "        if self.mlp_bias is not None:\n",
    "            # Concat constant zero-bias for qkv w/ trainable mlp_bias.\n",
    "            # Appears faster than adding to x_mlp separately\n",
    "            y = F.linear(y, self.in_proj.weight, torch.cat((self.qkv_bias, self.mlp_bias)))\n",
    "        else:\n",
    "            y = self.in_proj(y)\n",
    "        x_mlp, q, k, v = torch.split(y, self.in_split, dim=-1)\n",
    "\n",
    "        # Dot product attention w/ qk norm\n",
    "        q = self.q_norm(q.view(B, N, self.num_heads, self.head_dim)).transpose(1, 2)\n",
    "        k = self.k_norm(k.view(B, N, self.num_heads, self.head_dim)).transpose(1, 2)\n",
    "        v = v.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        if self.fused_attn:\n",
    "            x_attn = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p=self.attn_drop.p,\n",
    "            )\n",
    "        else:\n",
    "            q = q * self.scale\n",
    "            attn = q @ k.transpose(-2, -1)\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x_attn = attn @ v\n",
    "        x_attn = x_attn.transpose(1, 2).reshape(B, N, C)\n",
    "        x_attn = self.attn_out_proj(x_attn)\n",
    "\n",
    "        # MLP activation, dropout, fc2\n",
    "        x_mlp = self.mlp_act(x_mlp)\n",
    "        x_mlp = self.mlp_drop(x_mlp)\n",
    "        x_mlp = self.mlp_out_proj(x_mlp)\n",
    "\n",
    "        # Add residual w/ drop path & layer scale applied\n",
    "        y = self.drop_path(self.ls(x_attn + x_mlp))\n",
    "        x = x + y\n",
    "        return x\n",
    "\n",
    "\n",
    "class ParallelThingsBlock(nn.Module):\n",
    "    \"\"\" Parallel ViT block (N parallel attention followed by N parallel MLP)\n",
    "    Based on:\n",
    "      `Three things everyone should know about Vision Transformers` - https://arxiv.org/abs/2203.09795\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            num_parallel=2,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            init_values=None,\n",
    "            proj_drop=0.,\n",
    "            attn_drop=0.,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            mlp_layer=Mlp,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_parallel = num_parallel\n",
    "        self.attns = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        for _ in range(num_parallel):\n",
    "            self.attns.append(nn.Sequential(OrderedDict([\n",
    "                ('norm', norm_layer(dim)),\n",
    "                ('attn', Attention(\n",
    "                    dim,\n",
    "                    num_heads=num_heads,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_norm=qk_norm,\n",
    "                    attn_drop=attn_drop,\n",
    "                    proj_drop=proj_drop,\n",
    "                    norm_layer=norm_layer,\n",
    "                )),\n",
    "                ('ls', LayerScale(dim, init_values=init_values) if init_values else nn.Identity()),\n",
    "                ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())\n",
    "            ])))\n",
    "            self.ffns.append(nn.Sequential(OrderedDict([\n",
    "                ('norm', norm_layer(dim)),\n",
    "                ('mlp', mlp_layer(\n",
    "                    dim,\n",
    "                    hidden_features=int(dim * mlp_ratio),\n",
    "                    act_layer=act_layer,\n",
    "                    drop=proj_drop,\n",
    "                )),\n",
    "                ('ls', LayerScale(dim, init_values=init_values) if init_values else nn.Identity()),\n",
    "                ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())\n",
    "            ])))\n",
    "\n",
    "    def _forward_jit(self, x):\n",
    "        x = x + torch.stack([attn(x) for attn in self.attns]).sum(dim=0)\n",
    "        x = x + torch.stack([ffn(x) for ffn in self.ffns]).sum(dim=0)\n",
    "        return x\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def _forward(self, x):\n",
    "        x = x + sum(attn(x) for attn in self.attns)\n",
    "        x = x + sum(ffn(x) for ffn in self.ffns)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.jit.is_scripting() or torch.jit.is_tracing():\n",
    "            return self._forward_jit(x)\n",
    "        else:\n",
    "            return self._forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (64, 128)\n",
    "in_channels = 3\n",
    "out_channels = 3\n",
    "history=1\n",
    "patch_size=2\n",
    "drop_path=0.1\n",
    "drop_rate=0.1\n",
    "learn_pos_emb=True\n",
    "embed_dim=128\n",
    "depth=4\n",
    "decoder_depth=1\n",
    "num_heads=4\n",
    "mlp_ratio=4\n",
    "embed_layer = PatchEmbed\n",
    "pre_norm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim)\n",
    "num_patches = patch_embed.num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embed = nn.Parameter(\n",
    "    torch.zeros(1, num_patches, embed_dim), requires_grad=learn_pos_emb\n",
    ")\n",
    "pos_drop = nn.Dropout(p=drop_rate)\n",
    "dpr = [x.item() for x in torch.linspace(0, drop_path, depth)]\n",
    "blocks = nn.ModuleList(\n",
    "    [\n",
    "        Block(\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            mlp_ratio,\n",
    "            qkv_bias=True,\n",
    "            drop_path=dpr[i],\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            proj_drop=drop_rate,\n",
    "            attn_drop=drop_rate,\n",
    "        )\n",
    "        for i in range(depth)\n",
    "    ]\n",
    ")\n",
    "norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "head = nn.ModuleList()\n",
    "for _ in range(decoder_depth):\n",
    "    head.append(nn.Linear(embed_dim, embed_dim))\n",
    "    head.append(nn.GELU())\n",
    "head.append(nn.Linear(embed_dim, out_channels * patch_size**2))\n",
    "head = nn.Sequential(*head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpatchify(x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (B, L, V * patch_size**2)\n",
    "        return imgs: (B, V, H, W)\n",
    "        \"\"\"\n",
    "        p = patch_size\n",
    "        c = out_channels\n",
    "        h = img_size[0] // p\n",
    "        w = img_size[1] // p\n",
    "        assert h * w == x.shape[1]\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
    "        x = torch.einsum(\"nhwpqc->nchpwq\", x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], c, h * p, w * p))\n",
    "        return imgs\n",
    "\n",
    "def forward_encoder(self, x: torch.Tensor):\n",
    "    # x.shape = [B,C,H,W]\n",
    "    x = patch_embed(x)\n",
    "    # x.shape = [B,num_patches,embed_dim]\n",
    "    x = x + pos_embed\n",
    "    x = pos_drop(x)\n",
    "    for blk in blocks:\n",
    "        x = blk(x)\n",
    "    # x.shape = [B,num_patches,embed_dim]\n",
    "    x = norm(x)\n",
    "    return x\n",
    "\n",
    "def forward(self, x):\n",
    "        if len(x.shape) == 5:  # x.shape = [B,T,in_channels,H,W]\n",
    "            x = x.flatten(1, 2)\n",
    "        # x.shape = [B,T*in_channels,H,W]\n",
    "        x = self.forward_encoder(x)\n",
    "        # x.shape = [B,num_patches,embed_dim]\n",
    "        x = self.head(x)\n",
    "        # x.shape = [B,num_patches,embed_dim]\n",
    "        preds = self.unpatchify(x)\n",
    "        # preds.shape = [B,out_channels,H,W]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 64, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 32, 64])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size, bias=True)(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 2048])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size, bias=True)(x).flatten(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(x.shape) == 5:  # x.shape = [B,T,C,H,W]\n",
    "    x = x.flatten(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 128])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = patch_embed(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x + pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 128])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pos_drop(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 128])\n",
      "torch.Size([1, 2048, 128])\n",
      "torch.Size([1, 2048, 128])\n",
      "torch.Size([1, 2048, 128])\n"
     ]
    }
   ],
   "source": [
    "for blk in blocks:\n",
    "    x = blk(x)\n",
    "    print(x.shape)\n",
    "    # x.shape = [B,num_patches,embed_dim]\n",
    "x = norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 128])\n",
      "torch.Size([1, 2048, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 128])\n",
      "torch.Size([1, 2048, 128])\n"
     ]
    }
   ],
   "source": [
    "for block in blocks:\n",
    "    x = block(x)\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (1): GELU(approximate='none')\n",
       "  (2): Linear(in_features=128, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 12])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = head(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "preds = unpatchify(x)\n",
    "print(preds.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bias_correction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
