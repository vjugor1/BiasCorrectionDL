{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.jit import Final\n",
    "from timm.layers import (\n",
    "    Mlp,\n",
    "    Format,\n",
    "    PatchDropout,\n",
    "    LayerNorm2d,\n",
    "    RotaryEmbeddingCat,\n",
    "    to_2tuple,\n",
    "    nchw_to\n",
    ")\n",
    "\n",
    "# Standard library\n",
    "from functools import partial\n",
    "from typing import Callable, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Position Embedding Utilities\n",
    "\n",
    "Hacked together by / Copyright 2022 Ross Wightman\n",
    "\"\"\"\n",
    "import logging\n",
    "import math\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def resample_abs_pos_embed(\n",
    "        posemb,\n",
    "        new_size: List[int],\n",
    "        old_size: Optional[List[int]] = None,\n",
    "        num_prefix_tokens: int = 1,\n",
    "        interpolation: str = 'bicubic',\n",
    "        antialias: bool = True,\n",
    "        verbose: bool = False,\n",
    "):\n",
    "    # sort out sizes, assume square if old size not provided\n",
    "    num_pos_tokens = posemb.shape[1]\n",
    "    num_new_tokens = new_size[0] * new_size[1] + num_prefix_tokens\n",
    "    if num_new_tokens == num_pos_tokens and new_size[0] == new_size[1]:\n",
    "        return posemb\n",
    "\n",
    "    if old_size is None:\n",
    "        hw = int(math.sqrt(num_pos_tokens - num_prefix_tokens))\n",
    "        old_size = hw, hw\n",
    "\n",
    "    if num_prefix_tokens:\n",
    "        posemb_prefix, posemb = posemb[:, :num_prefix_tokens], posemb[:, num_prefix_tokens:]\n",
    "    else:\n",
    "        posemb_prefix, posemb = None, posemb\n",
    "\n",
    "    # do the interpolation\n",
    "    embed_dim = posemb.shape[-1]\n",
    "    orig_dtype = posemb.dtype\n",
    "    posemb = posemb.float()  # interpolate needs float32\n",
    "    posemb = posemb.reshape(1, old_size[0], old_size[1], -1).permute(0, 3, 1, 2)\n",
    "    posemb = F.interpolate(posemb, size=new_size, mode=interpolation, antialias=antialias)\n",
    "    posemb = posemb.permute(0, 2, 3, 1).reshape(1, -1, embed_dim)\n",
    "    posemb = posemb.to(orig_dtype)\n",
    "\n",
    "    # add back extra (class, etc) prefix tokens\n",
    "    if posemb_prefix is not None:\n",
    "        posemb = torch.cat([posemb_prefix, posemb], dim=1)\n",
    "\n",
    "    if not torch.jit.is_scripting() and verbose:\n",
    "        _logger.info(f'Resized position embedding: {old_size} to {new_size}.')\n",
    "\n",
    "    return posemb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch import _assert\n",
    "except ImportError:\n",
    "    def _assert(condition: bool, message: str):\n",
    "        assert condition, message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.jit import Final\n",
    "\n",
    "from timm.layers import (\n",
    "    Mlp,\n",
    "    DropPath,\n",
    "    apply_rot_embed_cat,\n",
    "    use_fused_attn,\n",
    ")\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" 2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    output_fmt: Format\n",
    "    dynamic_img_pad: torch.jit.Final[bool]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size: Optional[int] = 224,\n",
    "            patch_size: int = 16,\n",
    "            in_chans: int = 3,\n",
    "            embed_dim: int = 768,\n",
    "            norm_layer: Optional[Callable] = None,\n",
    "            flatten: bool = True,\n",
    "            output_fmt: Optional[str] = None,\n",
    "            bias: bool = True,\n",
    "            strict_img_size: bool = True,\n",
    "            dynamic_img_pad: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_size = to_2tuple(patch_size)\n",
    "        if img_size is not None:\n",
    "            self.img_size = to_2tuple(img_size)\n",
    "            self.grid_size = tuple([s // p for s, p in zip(self.img_size, self.patch_size)])\n",
    "            self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        else:\n",
    "            self.img_size = None\n",
    "            self.grid_size = None\n",
    "            self.num_patches = None\n",
    "\n",
    "        if output_fmt is not None:\n",
    "            self.flatten = False\n",
    "            self.output_fmt = Format(output_fmt)\n",
    "        else:\n",
    "            # flatten spatial dim and transpose to channels last, kept for bwd compat\n",
    "            self.flatten = flatten\n",
    "            self.output_fmt = Format.NCHW\n",
    "        self.strict_img_size = strict_img_size\n",
    "        self.dynamic_img_pad = dynamic_img_pad\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        if self.img_size is not None:\n",
    "            if self.strict_img_size:\n",
    "                _assert(H == self.img_size[0], f\"Input height ({H}) doesn't match model ({self.img_size[0]}).\")\n",
    "                _assert(W == self.img_size[1], f\"Input width ({W}) doesn't match model ({self.img_size[1]}).\")\n",
    "            elif not self.dynamic_img_pad:\n",
    "                _assert(\n",
    "                    H % self.patch_size[0] == 0,\n",
    "                    f\"Input height ({H}) should be divisible by patch size ({self.patch_size[0]}).\"\n",
    "                )\n",
    "                _assert(\n",
    "                    W % self.patch_size[1] == 0,\n",
    "                    f\"Input width ({W}) should be divisible by patch size ({self.patch_size[1]}).\"\n",
    "                )\n",
    "        if self.dynamic_img_pad:\n",
    "            pad_h = (self.patch_size[0] - H % self.patch_size[0]) % self.patch_size[0]\n",
    "            pad_w = (self.patch_size[1] - W % self.patch_size[1]) % self.patch_size[1]\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h))\n",
    "        x = self.proj(x)\n",
    "        if self.flatten:\n",
    "            x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC\n",
    "        elif self.output_fmt != Format.NCHW:\n",
    "            x = nchw_to(x, self.output_fmt)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "def get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get relative positional embeddings according to the relative positions of\n",
    "        query and key sizes.\n",
    "    Args:\n",
    "        q_size (int): size of query q.\n",
    "        k_size (int): size of key k.\n",
    "        rel_pos (Tensor): relative position embeddings (L, C).\n",
    "\n",
    "    Returns:\n",
    "        Extracted positional embeddings according to relative positions.\n",
    "    \"\"\"\n",
    "    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
    "    # Interpolate rel pos if needed.\n",
    "    if rel_pos.shape[0] != max_rel_dist:\n",
    "        # Interpolate rel pos.\n",
    "        rel_pos_resized = F.interpolate(\n",
    "            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\n",
    "            size=max_rel_dist,\n",
    "            mode=\"linear\",\n",
    "        )\n",
    "        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n",
    "    else:\n",
    "        rel_pos_resized = rel_pos\n",
    "\n",
    "    # Scale the coords with short length if shapes for q and k are different.\n",
    "    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
    "    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
    "    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
    "\n",
    "    return rel_pos_resized[relative_coords.long()]\n",
    "\n",
    "\n",
    "def get_decomposed_rel_pos_bias(\n",
    "    q: torch.Tensor,\n",
    "    rel_pos_h: torch.Tensor,\n",
    "    rel_pos_w: torch.Tensor,\n",
    "    q_size: Tuple[int, int],\n",
    "    k_size: Tuple[int, int],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n",
    "    https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py\n",
    "    Args:\n",
    "        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n",
    "        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n",
    "        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n",
    "        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n",
    "        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n",
    "\n",
    "    Returns:\n",
    "        bias (Tensor): attention bias to add to attention map\n",
    "    \"\"\"\n",
    "    q_h, q_w = q_size\n",
    "    k_h, k_w = k_size\n",
    "    Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n",
    "    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n",
    "\n",
    "    B, _, dim = q.shape\n",
    "    r_q = q.reshape(B, q_h, q_w, dim)\n",
    "    rel_h = torch.einsum(\"bhwc,hkc->bhwk\", r_q, Rh)\n",
    "    rel_w = torch.einsum(\"bhwc,wkc->bhwk\", r_q, Rw)\n",
    "\n",
    "    attn_bias = rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n",
    "    return attn_bias.reshape(-1, q_h * q_w, k_h * k_w)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    fused_attn: Final[bool]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads=8,\n",
    "        qkv_bias=True,\n",
    "        qk_norm=False,\n",
    "        attn_drop=0.0,\n",
    "        proj_drop=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        use_rel_pos: bool = False,\n",
    "        input_size: Optional[Tuple[int, int]] = None,\n",
    "        rope: Optional[nn.Module] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        self.fused_attn = use_fused_attn()\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.use_rel_pos = use_rel_pos\n",
    "        if self.use_rel_pos:\n",
    "            assert rope is None\n",
    "            assert (\n",
    "                input_size is not None\n",
    "            ), \"Input size must be provided if using relative positional encoding.\"\n",
    "            # initialize relative positional embeddings\n",
    "            self.rel_pos_h = nn.Parameter(\n",
    "                torch.zeros(2 * input_size[0] - 1, self.head_dim)\n",
    "            )\n",
    "            self.rel_pos_w = nn.Parameter(\n",
    "                torch.zeros(2 * input_size[1] - 1, self.head_dim)\n",
    "            )\n",
    "        self.rope = rope\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, _ = x.shape\n",
    "        N = H * W\n",
    "        x = x.reshape(B, N, -1)\n",
    "        qkv = self.qkv(x).view(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        # qkv with shape (3, B, nHead, H * W, C)\n",
    "        q, k, v = qkv.reshape(3, B * self.num_heads, N, -1).unbind(0)\n",
    "        # q, k, v with shape (B * nHead, H * W, C)\n",
    "        q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "        if self.use_rel_pos:\n",
    "            attn_bias = get_decomposed_rel_pos_bias(\n",
    "                q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W)\n",
    "            )\n",
    "        else:\n",
    "            attn_bias = None\n",
    "            if self.rope is not None:\n",
    "                rope = self.rope.get_embed()\n",
    "                q = apply_rot_embed_cat(q, rope).type_as(v)\n",
    "                k = apply_rot_embed_cat(k, rope).type_as(v)\n",
    "\n",
    "        if self.fused_attn:\n",
    "            x = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q,\n",
    "                k,\n",
    "                v,\n",
    "                attn_mask=attn_bias,\n",
    "                dropout_p=self.attn_drop.p if self.training else 0.0,\n",
    "            )\n",
    "        else:\n",
    "            q = q * self.scale\n",
    "            attn = q @ k.transpose(-2, -1)\n",
    "            if attn_bias is not None:\n",
    "                attn = attn + attn_bias\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x = attn @ v\n",
    "\n",
    "        x = x.view(B, self.num_heads, N, -1).transpose(1, 2).reshape(B, N, -1)\n",
    "        x = self.proj(x)\n",
    "        x = x.view(B, H, W, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, init_values=1e-5, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_norm=False,\n",
    "        proj_drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        init_values=None,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        mlp_layer=Mlp,\n",
    "        use_rel_pos=False,\n",
    "        window_size=0,\n",
    "        input_size=None,\n",
    "        rope=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_norm=qk_norm,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=proj_drop,\n",
    "            norm_layer=norm_layer,\n",
    "            use_rel_pos=use_rel_pos,\n",
    "            input_size=input_size if window_size == 0 else (window_size, window_size),\n",
    "            rope=rope,\n",
    "        )\n",
    "        self.ls1 = (\n",
    "            LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        )\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = mlp_layer(\n",
    "            in_features=dim,\n",
    "            hidden_features=int(dim * mlp_ratio),\n",
    "            act_layer=act_layer,\n",
    "            drop=proj_drop,\n",
    "        )\n",
    "        self.ls2 = (\n",
    "            LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        )\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, _ = x.shape\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        # Window partition\n",
    "        pad_hw: Optional[Tuple[int, int]] = None\n",
    "        if self.window_size > 0:\n",
    "            x, pad_hw = window_partition(x, self.window_size)\n",
    "\n",
    "        x = self.drop_path1(self.ls1(self.attn(x)))\n",
    "\n",
    "        # Reverse window partition\n",
    "        if self.window_size > 0:\n",
    "            x = window_unpartition(x, self.window_size, (H, W), pad_hw)\n",
    "\n",
    "        x = shortcut + x\n",
    "\n",
    "        x = x.reshape(B, H * W, -1)  # MLP is faster for N, L, C tensor\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        x = x.reshape(B, H, W, -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(\n",
    "    x: torch.Tensor, window_size: int\n",
    ") -> Tuple[torch.Tensor, Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Partition into non-overlapping windows with padding if needed.\n",
    "    Args:\n",
    "        x (tensor): input tokens with [B, H, W, C].\n",
    "        window_size (int): window size.\n",
    "\n",
    "    Returns:\n",
    "        windows: windows after partition with [B * num_windows, window_size, window_size, C].\n",
    "        (Hp, Wp): padded height and width before partition\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "\n",
    "    pad_h = (window_size - H % window_size) % window_size\n",
    "    pad_w = (window_size - W % window_size) % window_size\n",
    "    x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n",
    "    Hp, Wp = H + pad_h, W + pad_w\n",
    "\n",
    "    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n",
    "    windows = (\n",
    "        x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    )\n",
    "    return windows, (Hp, Wp)\n",
    "\n",
    "\n",
    "def window_unpartition(\n",
    "    windows: torch.Tensor,\n",
    "    window_size: int,\n",
    "    hw: Tuple[int, int],\n",
    "    pad_hw: Optional[Tuple[int, int]] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Window unpartition into original sequences and removing padding.\n",
    "    Args:\n",
    "        windows (tensor): input tokens with [B * num_windows, window_size, window_size, C].\n",
    "        window_size (int): window size.\n",
    "        pad_hw (Tuple): padded height and width (Hp, Wp).\n",
    "        hw (Tuple): original height and width (H, W) before padding.\n",
    "\n",
    "    Returns:\n",
    "        x: unpartitioned sequences with [B, H, W, C].\n",
    "    \"\"\"\n",
    "    Hp, Wp = pad_hw if pad_hw is not None else hw\n",
    "    H, W = hw\n",
    "    B = windows.shape[0] // (Hp * Wp // window_size // window_size)\n",
    "    x = windows.view(\n",
    "        B, Hp // window_size, Wp // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, -1)\n",
    "    x = x[:, :H, :W, :].contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def checkpoint_seq(\n",
    "        functions,\n",
    "        x,\n",
    "        every=1,\n",
    "        flatten=False,\n",
    "        skip_last=False,\n",
    "        preserve_rng_state=True\n",
    "):\n",
    "    r\"\"\"A helper function for checkpointing sequential models.\n",
    "\n",
    "    Sequential models execute a list of modules/functions in order\n",
    "    (sequentially). Therefore, we can divide such a sequence into segments\n",
    "    and checkpoint each segment. All segments except run in :func:`torch.no_grad`\n",
    "    manner, i.e., not storing the intermediate activations. The inputs of each\n",
    "    checkpointed segment will be saved for re-running the segment in the backward pass.\n",
    "\n",
    "    See :func:`~torch.utils.checkpoint.checkpoint` on how checkpointing works.\n",
    "\n",
    "    .. warning::\n",
    "        Checkpointing currently only supports :func:`torch.autograd.backward`\n",
    "        and only if its `inputs` argument is not passed. :func:`torch.autograd.grad`\n",
    "        is not supported.\n",
    "\n",
    "    .. warning:\n",
    "        At least one of the inputs needs to have :code:`requires_grad=True` if\n",
    "        grads are needed for model inputs, otherwise the checkpointed part of the\n",
    "        model won't have gradients.\n",
    "\n",
    "    Args:\n",
    "        functions: A :class:`torch.nn.Sequential` or the list of modules or functions to run sequentially.\n",
    "        x: A Tensor that is input to :attr:`functions`\n",
    "        every: checkpoint every-n functions (default: 1)\n",
    "        flatten (bool): flatten nn.Sequential of nn.Sequentials\n",
    "        skip_last (bool): skip checkpointing the last function in the sequence if True\n",
    "        preserve_rng_state (bool, optional, default=True):  Omit stashing and restoring\n",
    "            the RNG state during each checkpoint.\n",
    "\n",
    "    Returns:\n",
    "        Output of running :attr:`functions` sequentially on :attr:`*inputs`\n",
    "\n",
    "    Example:\n",
    "        >>> model = nn.Sequential(...)\n",
    "        >>> input_var = checkpoint_seq(model, input_var, every=2)\n",
    "    \"\"\"\n",
    "    def run_function(start, end, functions):\n",
    "        def forward(_x):\n",
    "            for j in range(start, end + 1):\n",
    "                _x = functions[j](_x)\n",
    "            return _x\n",
    "        return forward\n",
    "\n",
    "    if isinstance(functions, torch.nn.Sequential):\n",
    "        functions = functions.children()\n",
    "    if flatten:\n",
    "        functions = chain.from_iterable(functions)\n",
    "    if not isinstance(functions, (tuple, list)):\n",
    "        functions = tuple(functions)\n",
    "\n",
    "    num_checkpointed = len(functions)\n",
    "    if skip_last:\n",
    "        num_checkpointed -= 1\n",
    "    end = -1\n",
    "    for start in range(0, num_checkpointed, every):\n",
    "        end = min(start + every - 1, num_checkpointed - 1)\n",
    "        x = checkpoint(run_function(start, end, functions), x, preserve_rng_state=preserve_rng_state)\n",
    "    if skip_last:\n",
    "        return run_function(end + 1, len(functions) - 1, functions)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (64, 128)\n",
    "in_channels = 3\n",
    "out_channels = 3\n",
    "history=1\n",
    "patch_size=2\n",
    "drop_path=0.1\n",
    "drop_rate=0.1\n",
    "embed_dim=128\n",
    "depth=4\n",
    "decoder_depth=1\n",
    "num_heads=4\n",
    "mlp_ratio=4\n",
    "pre_norm = False\n",
    "\n",
    "\n",
    "qkv_bias: bool = True\n",
    "qk_norm: bool = False\n",
    "init_values: Optional[float] = None\n",
    "pos_drop_rate: float = 0.1\n",
    "patch_drop_rate: float = 0.0\n",
    "proj_drop_rate: float = 0.1\n",
    "attn_drop_rate: float = 0.1\n",
    "drop_path_rate: float = 0.1\n",
    "embed_layer:Callable = partial(\n",
    "                PatchEmbed, output_fmt=Format.NHWC, strict_img_size=False)\n",
    "norm_layer: Optional[Callable] = nn.LayerNorm\n",
    "act_layer: Optional[Callable] = nn.GELU\n",
    "block_fn: Callable = Block\n",
    "mlp_layer: Callable = Mlp\n",
    "use_abs_pos: bool = True\n",
    "use_rel_pos: bool = False\n",
    "use_rope: bool = False\n",
    "window_size: int = 14\n",
    "global_attn_indexes: Tuple[int, ...] = ()\n",
    "neck_chans: int = 0\n",
    "ref_feat_shape: Optional[Tuple[Tuple[int, int], Tuple[int, int]]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = img_size\n",
    "in_channels = in_channels * history\n",
    "out_channels = out_channels\n",
    "patch_size = patch_size\n",
    "norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "act_layer = act_layer or nn.GELU\n",
    "\n",
    "# num_features for consistency with other models\n",
    "num_features = embed_dim = embed_dim\n",
    "grad_checkpointing = False\n",
    "\n",
    "patch_embed = embed_layer(\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    in_chans=in_channels,\n",
    "    embed_dim=embed_dim,\n",
    "    bias=not pre_norm,  # disable bias if pre-norm is used\n",
    ")\n",
    "grid_size = patch_embed.grid_size\n",
    "\n",
    "if use_abs_pos:\n",
    "    # Initialize absolute positional embedding with pretrain image size.\n",
    "    pos_embed = nn.Parameter(torch.zeros(1, grid_size[0], grid_size[1], embed_dim))\n",
    "else:\n",
    "    pos_embed = None\n",
    "pos_drop = nn.Dropout(p=pos_drop_rate)\n",
    "if patch_drop_rate > 0:\n",
    "    patch_drop = PatchDropout(\n",
    "        patch_drop_rate,\n",
    "        num_prefix_tokens=0,\n",
    "    )\n",
    "else:\n",
    "    patch_drop = nn.Identity()\n",
    "norm_pre = norm_layer(embed_dim) if pre_norm else nn.Identity()\n",
    "\n",
    "if use_rope:\n",
    "    assert (\n",
    "        not use_rel_pos\n",
    "    ), \"ROPE and relative pos embeddings should not be enabled at same time\"\n",
    "    if ref_feat_shape is not None:\n",
    "        assert len(ref_feat_shape) == 2\n",
    "        ref_feat_shape_global = to_2tuple(ref_feat_shape[0])\n",
    "        ref_feat_shape_window = to_2tuple(ref_feat_shape[1])\n",
    "    else:\n",
    "        ref_feat_shape_global = ref_feat_shape_window = None\n",
    "    rope_global = RotaryEmbeddingCat(\n",
    "        embed_dim // num_heads,\n",
    "        in_pixels=False,\n",
    "        feat_shape=grid_size,\n",
    "        ref_feat_shape=ref_feat_shape_global,\n",
    "    )\n",
    "    rope_window = RotaryEmbeddingCat(\n",
    "        embed_dim // num_heads,\n",
    "        in_pixels=False,\n",
    "        feat_shape=to_2tuple(window_size),\n",
    "        ref_feat_shape=ref_feat_shape_window,\n",
    "    )\n",
    "else:\n",
    "    rope_global = None\n",
    "    rope_window = None\n",
    "\n",
    "# stochastic depth decay rule\n",
    "dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "blocks = nn.Sequential(\n",
    "    *[\n",
    "        block_fn(\n",
    "            dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_norm=qk_norm,\n",
    "            init_values=init_values,\n",
    "            proj_drop=proj_drop_rate,\n",
    "            attn_drop=attn_drop_rate,\n",
    "            drop_path=dpr[i],\n",
    "            norm_layer=norm_layer,\n",
    "            act_layer=act_layer,\n",
    "            mlp_layer=mlp_layer,\n",
    "            use_rel_pos=use_rel_pos,\n",
    "            window_size=window_size if i not in global_attn_indexes else 0,\n",
    "            input_size=grid_size,\n",
    "            rope=(\n",
    "                rope_window\n",
    "                if i not in global_attn_indexes\n",
    "                else rope_global\n",
    "            ),\n",
    "        )\n",
    "        for i in range(depth)\n",
    "    ]\n",
    ")\n",
    "if neck_chans:\n",
    "    neck = nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            embed_dim,\n",
    "            neck_chans,\n",
    "            kernel_size=1,\n",
    "            bias=False,\n",
    "        ),\n",
    "        LayerNorm2d(neck_chans),\n",
    "        nn.Conv2d(\n",
    "            neck_chans,\n",
    "            neck_chans,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        ),\n",
    "        LayerNorm2d(neck_chans),\n",
    "    )\n",
    "    num_features = neck_chans\n",
    "else:\n",
    "    neck = LayerNorm2d(embed_dim)\n",
    "    neck_chans = embed_dim\n",
    "\n",
    "head = nn.ModuleList()\n",
    "for _ in range(decoder_depth):\n",
    "    head.append(nn.Linear(neck_chans, neck_chans))\n",
    "    head.append(nn.GELU())\n",
    "head.append(nn.Linear(neck_chans, out_channels * patch_size**2))\n",
    "head = nn.Sequential(*head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpatchify(x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (B, Hp, Wp, V * patch_size**2) [1, 32, 64, 12]\n",
    "        return imgs: (B, V, H, W)\n",
    "        \"\"\"\n",
    "        p = patch_size\n",
    "        c = out_channels\n",
    "        h = img_size[0] // p\n",
    "        w = img_size[1] // p\n",
    "        assert h * w == x.shape[1] * x.shape[2]\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
    "        x = torch.einsum(\"nhwpqc->nchpwq\", x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], c, h * p, w * p))\n",
    "        return imgs\n",
    "\n",
    "def forward_encoder(self, x: torch.Tensor):\n",
    "    # x.shape = [B,C,H,W]\n",
    "    x = self.patch_embed(x)\n",
    "    # x.shape = [B,num_patches,embed_dim]\n",
    "    x = x + pos_embed\n",
    "    x = self.pos_drop(x)\n",
    "    x = self.patch_drop(x)\n",
    "    x = self.norm_pre(x)\n",
    "    if self.grad_checkpointing and not torch.jit.is_scripting():\n",
    "        x = checkpoint_seq(self.blocks, x)\n",
    "    else:\n",
    "        x = self.blocks(x)\n",
    "    x = self.neck(x.permute(0, 3, 1, 2))\n",
    "    # x.shape = [B,num_patches,embed_dim]\n",
    "    return x\n",
    "\n",
    "def forward(self, x):\n",
    "    if len(x.shape) == 5:  # x.shape = [B,T,in_channels,H,W]\n",
    "        x = x.flatten(1, 2)\n",
    "    # x.shape = [B,T*in_channels,H,W]\n",
    "    x = self.forward_encoder(x)\n",
    "    # x.shape = [B,num_patches,embed_dim]\n",
    "    x = self.head(x)\n",
    "    # x.shape = [B,num_patches,embed_dim]\n",
    "    preds = self.unpatchify(x)\n",
    "    # preds.shape = [B,out_channels,H,W]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 64, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(x.shape) == 5:  # x.shape = [B,T,C,H,W]\n",
    "    x = x.flatten(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 64, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = patch_embed(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 64, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x + pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 64, 128])\n",
      "torch.Size([1, 32, 64, 128])\n",
      "torch.Size([1, 32, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "x = pos_drop(x)\n",
    "print(x.shape)\n",
    "x = patch_drop(x)\n",
    "print(x.shape)\n",
    "x = norm_pre(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "x = blocks(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = neck(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 64, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (1): GELU(approximate='none')\n",
       "  (2): Linear(in_features=128, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 64, 12])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = head(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "preds = unpatchify(x)\n",
    "print(preds.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bias_correction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
